{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training Model\n",
    " Keras on Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import tensorflow as tf\n",
    "from skimage.io import imread, imsave, imshow\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import itertools\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from core.imageprep import dir_checker, random_crop, crop_generator, random_crop_batch\n",
    "from core.models import UNet, UNet_hp\n",
    "from core.metrics import iou_coef, dice_coef\n",
    "\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.summary import create_file_writer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython import get_ipython\n",
    "# %load_ext autoreload\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "# %autoreload 2\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "# %load_ext tensorboard\n",
    "get_ipython().run_line_magic('load_ext', 'tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "from packaging import version\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wucci_admin\\\\Anaconda3\\\\envs\\\\tfdl02\\\\python.exe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Images...\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2019_12_06_17_06\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2019_12_06_17_06\\images\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2019_12_06_17_06\\labels\n",
      "logs exists in D:PerlmutterData\n",
      "fit exists in D:PerlmutterData\\logs\n",
      "model exists in D:PerlmutterData\\logs\n",
      "pars exists in D:PerlmutterData\\logs\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "print(\"Load Images...\")\n",
    "# on mac\n",
    "# path = \"/Volumes/LaCie_DataStorage/PerlmutterData/\"\n",
    "\n",
    "# on Window PC \n",
    "path = os.path.join('D:', 'PerlmutterData')\n",
    "\n",
    "# input set\n",
    "crop_input_set = '2019_12_06_17_06'\n",
    "# crop_input_set = '2019_12_10_15_33_5x' # small training set\n",
    "\n",
    "imginput = os.path.join('dl_seg_project_raw', 'data_crop', crop_input_set,)\n",
    "imgpath = os.path.join(path, imginput)\n",
    "\n",
    "print(imgpath)\n",
    "\n",
    "img_dir = os.path.join(imgpath, 'images')\n",
    "label_dir = os.path.join(imgpath, 'labels')\n",
    "print(img_dir)\n",
    "print(label_dir)\n",
    "\n",
    "# check if the output folder exist. If not, create a folder\n",
    "dir_checker('logs', path)\n",
    "path_logs = os.path.join(path, 'logs')\n",
    "dir_checker('fit', path_logs)\n",
    "dir_checker('model', path_logs)\n",
    "dir_checker('pars', path_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Print the first file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2019_12_06_17_06\\images\\autophagosome\\0001.tif\n"
     ]
    }
   ],
   "source": [
    "imgpath_all = list(paths.list_images(imgpath))\n",
    "print(imgpath_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create Image Datagenerator\n",
    " 1. create only one datagen\n",
    " 2. specify valiation split in datagen argument\n",
    " 3. add split data when calling `datagen.flow_from_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "date =  datetime.now().strftime(\"%Y_%m_%d\")\n",
    "seed = 101\n",
    "batch_size = 16\n",
    "epoch = 100\n",
    "validation_steps = 20\n",
    "validation_split = 0.1\n",
    "training_sample_size = len(imgpath_all)\n",
    "IMG_HEIGHT = None\n",
    "IMG_WIDTH = None\n",
    "classes = ['cell_membrane', 'nucleus', 'autophagosome']\n",
    "inputclass = [classes[1]]\n",
    "learning_rate = 1e-5\n",
    "loss = \"binary_crossentropy\"\n",
    "metrics = ['accuracy', iou_coef, dice_coef]\n",
    "\n",
    "metrics_name = []\n",
    "for f in metrics:\n",
    "    if callable(f):\n",
    "        metrics_name.append(f.__name__)\n",
    "    else:\n",
    "        metrics_name.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nucleus exists in D:PerlmutterData\\logs\\pars\n",
      "2019_12_17 exists in D:PerlmutterData\\logs\\pars\\nucleus\n",
      "{'IMG_HEIGHT': None,\n",
      " 'IMG_WIDTH': None,\n",
      " 'batch_size': 16,\n",
      " 'classes': ['cell_membrane', 'nucleus', 'autophagosome'],\n",
      " 'crop_input_set': '2019_12_06_17_06',\n",
      " 'data_gen_img_args': {'height_shift_range': 0.1,\n",
      "                       'horizontal_flip': True,\n",
      "                       'rescale': 0.00392156862745098,\n",
      "                       'rotation_range': 90.0,\n",
      "                       'shear_range': 0.07,\n",
      "                       'validation_split': 0.1,\n",
      "                       'vertical_flip': True,\n",
      "                       'width_shift_range': 0.1,\n",
      "                       'zoom_range': 0.2},\n",
      " 'data_gen_label_args': {'height_shift_range': 0.1,\n",
      "                         'horizontal_flip': True,\n",
      "                         'rescale': 0.00392156862745098,\n",
      "                         'rotation_range': 90.0,\n",
      "                         'shear_range': 0.07,\n",
      "                         'validation_split': 0.1,\n",
      "                         'vertical_flip': True,\n",
      "                         'width_shift_range': 0.1,\n",
      "                         'zoom_range': 0.2},\n",
      " 'date': '2019_12_17',\n",
      " 'epoch': 100,\n",
      " 'inputclass': ['nucleus'],\n",
      " 'learning_rate': 1e-05,\n",
      " 'loss': 'binary_crossentropy',\n",
      " 'metrics_name': ['accuracy', 'iou_coef', 'dice_coef'],\n",
      " 'seed': 101,\n",
      " 'timestamp': '2019_12_17_13_37',\n",
      " 'training_sample_size': 54100,\n",
      " 'validation_split': 0.1,\n",
      " 'validation_steps': 20}\n"
     ]
    }
   ],
   "source": [
    "# create arguments for data generator\n",
    "data_gen_img_args = dict(\n",
    "                # featurewise_center = True,\n",
    "                # featurewise_std_normalization = True,\n",
    "                horizontal_flip = True,\n",
    "                vertical_flip = True,\n",
    "                rotation_range = 90.,\n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1,\n",
    "                shear_range = 0.07,\n",
    "                zoom_range = 0.2,\n",
    "                validation_split = validation_split, # <- specify validation_split ratio\n",
    "                # fill_mode='constant',\n",
    "                # cval=0.,\n",
    "                rescale=1.0/255.0,\n",
    "                )\n",
    "\n",
    "data_gen_label_args = dict(\n",
    "                # featurewise_center=True,\n",
    "                # featurewise_std_normalization=True,\n",
    "                horizontal_flip = True,\n",
    "                vertical_flip = True,\n",
    "                rotation_range = 90.,\n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1,\n",
    "                shear_range = 0.07,\n",
    "                zoom_range = 0.2,\n",
    "                validation_split = validation_split, # <- specify validation_split ratio\n",
    "                # fill_mode='constant',\n",
    "                # cval=0.,\n",
    "                # rescale=1.0/255.0,\n",
    "                rescale=1.0/255.0,\n",
    "                )\n",
    "\n",
    "# create parameter\n",
    "pars = dict(\n",
    "                # basic information\n",
    "                timestamp = timestamp,\n",
    "                date = date,\n",
    "                seed = seed,\n",
    "                batch_size = batch_size,\n",
    "                \n",
    "                # Data generator\n",
    "                crop_input_set = crop_input_set,\n",
    "                validation_steps = validation_steps,\n",
    "                validation_split = validation_split,\n",
    "                training_sample_size = training_sample_size,\n",
    "                \n",
    "                # training class\n",
    "                classes = classes,\n",
    "                inputclass = inputclass,\n",
    "    \n",
    "                # add datagen args\n",
    "                data_gen_img_args = data_gen_img_args,\n",
    "                data_gen_label_args = data_gen_label_args,\n",
    "                \n",
    "                # Build model\n",
    "                IMG_HEIGHT = IMG_HEIGHT,\n",
    "                IMG_WIDTH = IMG_WIDTH,\n",
    "                epoch = epoch, \n",
    "                loss = loss,\n",
    "                metrics_name = metrics_name,\n",
    "                learning_rate = learning_rate,\n",
    "                )\n",
    "\n",
    "# save parameter\n",
    "path_pars = os.path.join(path_logs, 'pars')\n",
    "dir_checker(inputclass[0], path_pars)\n",
    "dir_checker(date, os.path.join(path_pars, inputclass[0]))\n",
    "\n",
    "pprint(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:PerlmutterData\\logs\\pars\\nucleus\\2019_12_17\\pars_2019_12_17_13_37.json\n"
     ]
    }
   ],
   "source": [
    "par_file_dir = os.path.join(path_pars, inputclass[0], date, 'pars_' + timestamp + '.json')\n",
    "print(par_file_dir)\n",
    "\n",
    "with open(par_file_dir, 'w') as outfile:\n",
    "    json.dump(pars, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator\n",
    "image_datagen = ImageDataGenerator(**data_gen_img_args)\n",
    "label_datagen = ImageDataGenerator(**data_gen_label_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10395 images belonging to 1 classes.\n",
      "Found 10395 images belonging to 1 classes.\n",
      "Found 1155 images belonging to 1 classes.\n",
      "Found 1155 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# load images into generator\n",
    "train_image_generator = image_datagen.flow_from_directory(\n",
    "    img_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='training', # <- define subset as 'training'\n",
    "    seed=seed)\n",
    "\n",
    "train_label_generator = label_datagen.flow_from_directory(\n",
    "    label_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "valid_image_generator = image_datagen.flow_from_directory(\n",
    "    img_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation', # <- define subset as 'validation'\n",
    "    seed=seed)\n",
    "\n",
    "valid_label_generator = label_datagen.flow_from_directory(\n",
    "    label_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge image and label generator\n",
    "def combine_generator(gen1, gen2):\n",
    "    while True:\n",
    "        yield(gen1.next(), gen2.next()) \n",
    "train_generator = combine_generator(train_image_generator, train_label_generator)\n",
    "valid_generator = combine_generator(valid_image_generator, valid_label_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_generator = zip(train_image_generator, train_label_generator)\\nvalid_generator = zip(valid_image_generator, valid_label_generator)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_generator = zip(train_image_generator, train_label_generator)\n",
    "valid_generator = zip(valid_image_generator, valid_label_generator)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 3043.0\n"
     ]
    }
   ],
   "source": [
    "# calculate steps_per_epoch\n",
    "steps_per_epoch = training_sample_size * (1 - validation_split) // batch_size\n",
    "print(\"Steps per epoch: {}\".format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlogdir_tb = os.path.join(path_logs, 'fit', inputclass[0], date)\\nip= 'localhost'\\nport = '6006'\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "logdir_tb = os.path.join(path_logs, 'fit', inputclass[0], date)\n",
    "ip= 'localhost'\n",
    "port = '6006'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%tensorboard --logdir {logdir_tb} --host {ip}\\n# get_ipython().run_line_magic('tensorboard', '--logdir', )\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%tensorboard --logdir {logdir_tb} --host {ip}\n",
    "# get_ipython().run_line_magic('tensorboard', '--logdir', )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "# Create a .v2 file for saving hyperparameter and evaluation\n",
    "# so we can see the results on tensorboard\n",
    "hparamtuning_dir = os.path.join(path_logs, 'fit', inputclass[0], date, timestamp)\n",
    "\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.7, 0.8))\n",
    "hparams_list = [HP_DROPOUT]\n",
    "\n",
    "with tf.summary.create_file_writer(hparamtuning_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_DROPOUT],\n",
    "    metrics=[hp.Metric('accuracy', display_name='Accuracy'), \n",
    "             hp.Metric('iou_coef', display_name='IoU_Coef'), # create container for customized metrics\n",
    "             hp.Metric('dice_coef', display_name='Dice_Coef')], # the same\n",
    "  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_name, hparamtuning_dir, hparams):\n",
    "    \n",
    "    # checkpoint\n",
    "    modelfilename = 'model_' + timestamp + '.h5'\n",
    "    dir_checker(run_name, hparamtuning_dir)\n",
    "    dir_checker('model', os.path.join(hparamtuning_dir, run_name))\n",
    "    \n",
    "    modelfile_path = os.path.join(hparamtuning_dir, run_name, 'model', modelfilename)\n",
    "    checkpointer = ModelCheckpoint(filepath = modelfile_path, \n",
    "                                   monitor = 'val_loss', \n",
    "                                   mode = 'min', \n",
    "                                   verbose = 1, \n",
    "                                   save_best_only = True)\n",
    "\n",
    "    # early stopping \n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=8,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4)\n",
    "\n",
    "    # learning rate adjustment\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                        factor=0.1,\n",
    "                        patience=4,\n",
    "                        verbose=1,\n",
    "                        min_delta=1e-4)\n",
    "\n",
    "    # tensorboard\n",
    "    # file_writer = create_file_writer(os.path.join(path_logs, 'fit', inputclass[0], date, timestamp, \"metrics\"))\n",
    "    # file_writer.set_as_default()\n",
    "\n",
    "    metrics = ['accuracy', iou_coef, dice_coef]\n",
    "    \n",
    "    tensorboard_callback = TensorBoard(log_dir = os.path.join(hparamtuning_dir, run_name), \n",
    "                                       profile_batch = 0, \n",
    "                                       update_freq= 30,\n",
    "                                       histogram_freq = 1\n",
    "                                       )\n",
    "\n",
    "    # compile callbacks\n",
    "    # callbacks = [checkpointer, tensorboard_callback, early_stopping, reduceLR]\n",
    "    callbacks = [checkpointer, reduceLR, tensorboard_callback]\n",
    "    \n",
    "    hparamtuning_runname_dir = os.path.join(hparamtuning_dir, run_name)\n",
    "    \n",
    "    with tf.summary.create_file_writer(hparamtuning_runname_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "\n",
    "        # prepare the model\n",
    "        unetmodel = UNet_hp(shape = (IMG_HEIGHT, IMG_WIDTH), \n",
    "                            lr = learning_rate, \n",
    "                            loss = loss,\n",
    "                            metrics = metrics,\n",
    "                            hparams = hparams,\n",
    "                            hparams_list = hparams_list, \n",
    "                           )\n",
    "        \n",
    "        # load weight\n",
    "        path_model = os.path.join('D:', 'PerlmutterData', 'logs', 'model', \n",
    "                                'nucleus', \n",
    "                                '2019_12_13',\n",
    "                                '2019_12_13_17_28',\n",
    "                                'run-1', \n",
    "                                'model_2019_12_13_17_28.h5',)\n",
    "        \n",
    "        unetmodel.load_weights(path_model)\n",
    "        \n",
    "        # train the model\n",
    "        unetmodel.fit_generator(\n",
    "                            generator = train_generator, \n",
    "                            validation_data = valid_generator,\n",
    "                            validation_steps = validation_steps,\n",
    "                            steps_per_epoch = steps_per_epoch,\n",
    "                            epochs = epoch,  \n",
    "                            callbacks = callbacks,\n",
    "                            verbose = 1, \n",
    "                            )\n",
    "    \n",
    "        _, accuracy, iou, dice,  = unetmodel.evaluate_generator(valid_generator, steps = 50, verbose=1)\n",
    "        tf.summary.scalar('accuracy', accuracy, step = 1)\n",
    "        tf.summary.scalar('iou_coef', iou, step = 1)\n",
    "        tf.summary.scalar('dice_coef', dice, step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{HParam(name='dropout', domain=RealInterval(0.7, 0.8), display_name=None, description=None): 0.7}\n",
      "--- Starting trial: run-0\n",
      "{'dropout': 0.7}\n",
      "run-0 does not exist in D:PerlmutterData\\logs\\fit\\nucleus\\2019_12_17\\2019_12_17_13_37\n",
      "model does not exist in D:PerlmutterData\\logs\\fit\\nucleus\\2019_12_17\\2019_12_17_13_37\\run-0\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 6 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 6 36928       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, None, None, 6 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 1 73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 147584      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 2 295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 2 590080      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 2 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 5 1180160     max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 5 2359808     conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, None, 5 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 5 0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 1 4719616     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 9438208     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, None, 1 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, None, None, 1 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 5 2097664     up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 1 0           dropout[0][0]                    \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 5 4719104     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 5 2359808     conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 5 0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 2 524544      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 5 0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 2 1179904     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 2 590080      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, None, None, 2 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 1 131200      up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 2 0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 1 295040      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 1 147584      conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, None, None, 1 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 6 32832       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 1 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, None, None, 6 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, None, None, 6 36928       conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, None, None, 1 65          conv2d_21[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,030,593\n",
      "Trainable params: 31,030,593\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "   1/3043 [..............................] - ETA: 6:06:46 - loss: 0.0202 - accuracy: 0.9906 - iou_coef: 0.7675 - dice_coef: 0.7775WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9932 - iou_coef: 0.4861 - dice_coef: 0.4972\n",
      "Epoch 00001: val_loss improved from inf to 0.10595, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2019_12_17\\2019_12_17_13_37\\run-0\\model\\model_2019_12_17_13_37.h5\n",
      "3043/3043 [==============================] - 3002s 987ms/step - loss: 0.0170 - accuracy: 0.9932 - iou_coef: 0.4860 - dice_coef: 0.4972 - val_loss: 0.1059 - val_accuracy: 0.9670 - val_iou_coef: 0.5774 - val_dice_coef: 0.6026\n",
      "Epoch 2/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9944 - iou_coef: 0.5064 - dice_coef: 0.5166\n",
      "Epoch 00002: val_loss did not improve from 0.10595\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0130 - accuracy: 0.9944 - iou_coef: 0.5064 - dice_coef: 0.5166 - val_loss: 1.4223 - val_accuracy: 0.7939 - val_iou_coef: 0.3285 - val_dice_coef: 0.3809\n",
      "Epoch 3/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9944 - iou_coef: 0.5054 - dice_coef: 0.5155\n",
      "Epoch 00003: val_loss did not improve from 0.10595\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0131 - accuracy: 0.9944 - iou_coef: 0.5054 - dice_coef: 0.5154 - val_loss: 0.6736 - val_accuracy: 0.8743 - val_iou_coef: 0.4876 - val_dice_coef: 0.5425\n",
      "Epoch 4/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9946 - iou_coef: 0.5154 - dice_coef: 0.5255\n",
      "Epoch 00004: val_loss did not improve from 0.10595\n",
      "3043/3043 [==============================] - 2992s 983ms/step - loss: 0.0125 - accuracy: 0.9946 - iou_coef: 0.5154 - dice_coef: 0.5255 - val_loss: 0.4547 - val_accuracy: 0.8917 - val_iou_coef: 0.5853 - val_dice_coef: 0.6344\n",
      "Epoch 5/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9947 - iou_coef: 0.5175 - dice_coef: 0.5272\n",
      "Epoch 00005: val_loss did not improve from 0.10595\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "3043/3043 [==============================] - 2996s 984ms/step - loss: 0.0120 - accuracy: 0.9947 - iou_coef: 0.5176 - dice_coef: 0.5273 - val_loss: 0.1555 - val_accuracy: 0.9602 - val_iou_coef: 0.6196 - val_dice_coef: 0.6475\n",
      "Epoch 6/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9955 - iou_coef: 0.5381 - dice_coef: 0.5470\n",
      "Epoch 00006: val_loss did not improve from 0.10595\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0095 - accuracy: 0.9955 - iou_coef: 0.5381 - dice_coef: 0.5470 - val_loss: 0.1556 - val_accuracy: 0.9594 - val_iou_coef: 0.6233 - val_dice_coef: 0.6537\n",
      "Epoch 7/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9956 - iou_coef: 0.5407 - dice_coef: 0.5498\n",
      "Epoch 00007: val_loss did not improve from 0.10595\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0093 - accuracy: 0.9956 - iou_coef: 0.5407 - dice_coef: 0.5498 - val_loss: 0.1788 - val_accuracy: 0.9518 - val_iou_coef: 0.5913 - val_dice_coef: 0.6256\n",
      "Epoch 8/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9956 - iou_coef: 0.5459 - dice_coef: 0.5549\n",
      "Epoch 00008: val_loss did not improve from 0.10595\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0093 - accuracy: 0.9956 - iou_coef: 0.5460 - dice_coef: 0.5550 - val_loss: 0.1572 - val_accuracy: 0.9576 - val_iou_coef: 0.6447 - val_dice_coef: 0.6764\n",
      "Epoch 9/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9957 - iou_coef: 0.5507 - dice_coef: 0.5596\n",
      "Epoch 00009: val_loss did not improve from 0.10595\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0091 - accuracy: 0.9957 - iou_coef: 0.5507 - dice_coef: 0.5596 - val_loss: 0.1481 - val_accuracy: 0.9624 - val_iou_coef: 0.6385 - val_dice_coef: 0.6648\n",
      "Epoch 10/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9958 - iou_coef: 0.5529 - dice_coef: 0.5617\n",
      "Epoch 00010: val_loss did not improve from 0.10595\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0089 - accuracy: 0.9958 - iou_coef: 0.5528 - dice_coef: 0.5617 - val_loss: 0.1258 - val_accuracy: 0.9665 - val_iou_coef: 0.6678 - val_dice_coef: 0.6934\n",
      "Epoch 11/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5523 - dice_coef: 0.5612\n",
      "Epoch 00011: val_loss improved from 0.10595 to 0.09538, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2019_12_17\\2019_12_17_13_37\\run-0\\model\\model_2019_12_17_13_37.h5\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5523 - dice_coef: 0.5612 - val_loss: 0.0954 - val_accuracy: 0.9722 - val_iou_coef: 0.6670 - val_dice_coef: 0.6896\n",
      "Epoch 12/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5524 - dice_coef: 0.5612\n",
      "Epoch 00012: val_loss did not improve from 0.09538\n",
      "3043/3043 [==============================] - 2997s 985ms/step - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5524 - dice_coef: 0.5611 - val_loss: 0.1393 - val_accuracy: 0.9654 - val_iou_coef: 0.6435 - val_dice_coef: 0.6710\n",
      "Epoch 13/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5529 - dice_coef: 0.5617\n",
      "Epoch 00013: val_loss did not improve from 0.09538\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5529 - dice_coef: 0.5617 - val_loss: 0.1588 - val_accuracy: 0.9620 - val_iou_coef: 0.6704 - val_dice_coef: 0.6983\n",
      "Epoch 14/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9958 - iou_coef: 0.5532 - dice_coef: 0.5620\n",
      "Epoch 00014: val_loss did not improve from 0.09538\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0089 - accuracy: 0.9958 - iou_coef: 0.5532 - dice_coef: 0.5619 - val_loss: 0.1171 - val_accuracy: 0.9690 - val_iou_coef: 0.6702 - val_dice_coef: 0.6958\n",
      "Epoch 15/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5522 - dice_coef: 0.5611\n",
      "Epoch 00015: val_loss did not improve from 0.09538\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "3043/3043 [==============================] - 2992s 983ms/step - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5522 - dice_coef: 0.5611 - val_loss: 0.1021 - val_accuracy: 0.9747 - val_iou_coef: 0.6788 - val_dice_coef: 0.7009\n",
      "Epoch 16/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5558 - dice_coef: 0.5646\n",
      "Epoch 00016: val_loss improved from 0.09538 to 0.09445, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2019_12_17\\2019_12_17_13_37\\run-0\\model\\model_2019_12_17_13_37.h5\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5558 - dice_coef: 0.5646 - val_loss: 0.0944 - val_accuracy: 0.9746 - val_iou_coef: 0.6769 - val_dice_coef: 0.6999\n",
      "Epoch 17/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9958 - iou_coef: 0.5531 - dice_coef: 0.5617\n",
      "Epoch 00017: val_loss improved from 0.09445 to 0.08887, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2019_12_17\\2019_12_17_13_37\\run-0\\model\\model_2019_12_17_13_37.h5\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0087 - accuracy: 0.9958 - iou_coef: 0.5530 - dice_coef: 0.5617 - val_loss: 0.0889 - val_accuracy: 0.9745 - val_iou_coef: 0.6731 - val_dice_coef: 0.6959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5552 - dice_coef: 0.5640\n",
      "Epoch 00018: val_loss did not improve from 0.08887\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5551 - dice_coef: 0.5639 - val_loss: 0.0896 - val_accuracy: 0.9757 - val_iou_coef: 0.6222 - val_dice_coef: 0.6417\n",
      "Epoch 19/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5534 - dice_coef: 0.5621\n",
      "Epoch 00019: val_loss did not improve from 0.08887\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5533 - dice_coef: 0.5620 - val_loss: 0.1011 - val_accuracy: 0.9729 - val_iou_coef: 0.6504 - val_dice_coef: 0.6715\n",
      "Epoch 20/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5533 - dice_coef: 0.5620\n",
      "Epoch 00020: val_loss improved from 0.08887 to 0.08125, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2019_12_17\\2019_12_17_13_37\\run-0\\model\\model_2019_12_17_13_37.h5\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5533 - dice_coef: 0.5620 - val_loss: 0.0812 - val_accuracy: 0.9761 - val_iou_coef: 0.6471 - val_dice_coef: 0.6695\n",
      "Epoch 21/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5549 - dice_coef: 0.5637\n",
      "Epoch 00021: val_loss did not improve from 0.08125\n",
      "3043/3043 [==============================] - 2992s 983ms/step - loss: 0.0089 - accuracy: 0.9957 - iou_coef: 0.5549 - dice_coef: 0.5637 - val_loss: 0.1178 - val_accuracy: 0.9703 - val_iou_coef: 0.6739 - val_dice_coef: 0.6980\n",
      "Epoch 22/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9957 - iou_coef: 0.5526 - dice_coef: 0.5614\n",
      "Epoch 00022: val_loss did not improve from 0.08125\n",
      "3043/3043 [==============================] - 2993s 984ms/step - loss: 0.0088 - accuracy: 0.9957 - iou_coef: 0.5526 - dice_coef: 0.5614 - val_loss: 0.0981 - val_accuracy: 0.9720 - val_iou_coef: 0.6462 - val_dice_coef: 0.6666\n",
      "Epoch 23/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5532 - dice_coef: 0.5620\n",
      "Epoch 00023: val_loss did not improve from 0.08125\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5532 - dice_coef: 0.5620 - val_loss: 0.0829 - val_accuracy: 0.9756 - val_iou_coef: 0.6511 - val_dice_coef: 0.6734\n",
      "Epoch 24/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9958 - iou_coef: 0.5511 - dice_coef: 0.5598\n",
      "Epoch 00024: val_loss did not improve from 0.08125\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
      "3043/3043 [==============================] - 2994s 984ms/step - loss: 0.0089 - accuracy: 0.9958 - iou_coef: 0.5512 - dice_coef: 0.5599 - val_loss: 0.1050 - val_accuracy: 0.9715 - val_iou_coef: 0.6550 - val_dice_coef: 0.6795\n",
      "Epoch 25/100\n",
      "3042/3043 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5518 - dice_coef: 0.5606\n",
      "Epoch 00025: val_loss did not improve from 0.08125\n",
      "3043/3043 [==============================] - 2995s 984ms/step - loss: 0.0088 - accuracy: 0.9958 - iou_coef: 0.5518 - dice_coef: 0.5607 - val_loss: 0.0989 - val_accuracy: 0.9736 - val_iou_coef: 0.6481 - val_dice_coef: 0.6718\n",
      "Epoch 26/100\n",
      "1571/3043 [==============>...............] - ETA: 24:02 - loss: 0.0087 - accuracy: 0.9958 - iou_coef: 0.5531 - dice_coef: 0.5620"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "    hparams = {\n",
    "      HP_DROPOUT: dropout_rate,\n",
    "    }\n",
    "    print(hparams)\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    \n",
    "    run(run_name, hparamtuning_dir, hparams)\n",
    "    session_num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "tfdl02",
   "language": "python",
   "name": "tfdl02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
