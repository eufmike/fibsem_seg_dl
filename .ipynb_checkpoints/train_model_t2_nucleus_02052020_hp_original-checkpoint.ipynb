{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model\n",
    "1. Keras on Tensorflow\n",
    "2. hyper-parameter search\n",
    "3. vanila-Unet: pull the hyper parameter setting out from the function of model\n",
    "4. date: 01312020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "from skimage.io import imread, imsave, imshow\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import itertools\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from core.imageprep import dir_checker, random_crop, crop_generator, random_crop_batch\n",
    "from core.models import UNet, UNet_hp, vanilla_unet, vanilla_unet_nodrop\n",
    "from core.metrics import iou_coef, dice_coef\n",
    "\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython import get_ipython\n",
    "# %load_ext autoreload\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "# %autoreload 2\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "# %load_ext tensorboard\n",
    "get_ipython().run_line_magic('load_ext', 'tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "from packaging import version\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wucci_admin\\\\Anaconda3\\\\envs\\\\tfdl02\\\\python.exe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Images...\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_30_11_24_1x\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_30_11_24_1x\\images\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_30_11_24_1x\\labels\n",
      "logs exists in D:PerlmutterData\n",
      "fit exists in D:PerlmutterData\\logs\n",
      "model exists in D:PerlmutterData\\logs\n",
      "pars exists in D:PerlmutterData\\logs\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "print(\"Load Images...\")\n",
    "# on mac\n",
    "# path = \"/Volumes/LaCie_DataStorage/PerlmutterData/\"\n",
    "\n",
    "# on Window PC \n",
    "path = os.path.join('D:', 'PerlmutterData')\n",
    "\n",
    "# input set\n",
    "crop_input_set = '2020_01_23_09_51_20x'\n",
    "# crop_input_set = '2020_01_30_11_24_1x' # small training set\n",
    "\n",
    "imginput = os.path.join('dl_seg_project_raw', 'data_crop', crop_input_set,)\n",
    "imgpath = os.path.join(path, imginput)\n",
    "\n",
    "print(imgpath)\n",
    "\n",
    "img_dir = os.path.join(imgpath, 'images')\n",
    "label_dir = os.path.join(imgpath, 'labels')\n",
    "print(img_dir)\n",
    "print(label_dir)\n",
    "\n",
    "# check if the output folder exist. If not, create a folder\n",
    "dir_checker('logs', path)\n",
    "path_logs = os.path.join(path, 'logs')\n",
    "dir_checker('fit', path_logs)\n",
    "dir_checker('model', path_logs)\n",
    "dir_checker('pars', path_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Print the first file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_30_11_24_1x\\images\\nucleus\\0001.tif\n"
     ]
    }
   ],
   "source": [
    "imgpath_all = list(paths.list_images(imgpath))\n",
    "print(imgpath_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create Image Datagenerator\n",
    " 1. create only one datagen\n",
    " 2. specify valiation split in datagen argument\n",
    " 3. add split data when calling `datagen.flow_from_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "date =  datetime.now().strftime(\"%Y_%m_%d\")\n",
    "seed = 103\n",
    "batch_size = 16\n",
    "epoch = 20\n",
    "validation_steps = 20\n",
    "validation_split = 0.1\n",
    "training_sample_size = len(imgpath_all)\n",
    "IMG_HEIGHT = None\n",
    "IMG_WIDTH = None\n",
    "classes = ['cell_membrane', 'nucleus', 'autophagosome']\n",
    "inputclass = [classes[1]]\n",
    "learning_rate = 1e-4\n",
    "loss = \"binary_crossentropy\"\n",
    "metrics = ['accuracy', iou_coef, dice_coef]\n",
    "\n",
    "metrics_name = []\n",
    "for f in metrics:\n",
    "    if callable(f):\n",
    "        metrics_name.append(f.__name__)\n",
    "    else:\n",
    "        metrics_name.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nucleus exists in D:PerlmutterData\\logs\\pars\n",
      "2020_02_05 exists in D:PerlmutterData\\logs\\pars\\nucleus\n",
      "{'IMG_HEIGHT': None,\n",
      " 'IMG_WIDTH': None,\n",
      " 'batch_size': 16,\n",
      " 'classes': ['cell_membrane', 'nucleus', 'autophagosome'],\n",
      " 'crop_input_set': '2020_01_30_11_24_1x',\n",
      " 'data_gen_img_args': {'height_shift_range': 0.1,\n",
      "                       'horizontal_flip': True,\n",
      "                       'rescale': 0.00392156862745098,\n",
      "                       'rotation_range': 90.0,\n",
      "                       'shear_range': 0.07,\n",
      "                       'validation_split': 0.3,\n",
      "                       'vertical_flip': True,\n",
      "                       'width_shift_range': 0.1,\n",
      "                       'zoom_range': 0.2},\n",
      " 'data_gen_label_args': {'height_shift_range': 0.1,\n",
      "                         'horizontal_flip': True,\n",
      "                         'rescale': 0.00392156862745098,\n",
      "                         'rotation_range': 90.0,\n",
      "                         'shear_range': 0.07,\n",
      "                         'validation_split': 0.3,\n",
      "                         'vertical_flip': True,\n",
      "                         'width_shift_range': 0.1,\n",
      "                         'zoom_range': 0.2},\n",
      " 'date': '2020_02_05',\n",
      " 'epoch': 20,\n",
      " 'inputclass': ['nucleus'],\n",
      " 'learning_rate': 0.0001,\n",
      " 'loss': 'binary_crossentropy',\n",
      " 'metrics_name': ['accuracy', 'iou_coef', 'dice_coef'],\n",
      " 'seed': 103,\n",
      " 'timestamp': '2020_02_05_16_27',\n",
      " 'training_sample_size': 662,\n",
      " 'validation_split': 0.3,\n",
      " 'validation_steps': 20}\n"
     ]
    }
   ],
   "source": [
    "# create arguments for data generator\n",
    "data_gen_img_args = dict(\n",
    "                # featurewise_center = True,\n",
    "                # featurewise_std_normalization = True,\n",
    "                horizontal_flip = True,\n",
    "                vertical_flip = True,\n",
    "                rotation_range = 90.,\n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1,\n",
    "                shear_range = 0.07,\n",
    "                zoom_range = 0.2,\n",
    "                validation_split = validation_split, # <- specify validation_split ratio\n",
    "                # fill_mode='constant',\n",
    "                # cval=0.,\n",
    "                rescale=1.0/255.0,\n",
    "                )\n",
    "\n",
    "data_gen_label_args = dict(\n",
    "                # featurewise_center=True,\n",
    "                # featurewise_std_normalization=True,\n",
    "                horizontal_flip = True,\n",
    "                vertical_flip = True,\n",
    "                rotation_range = 90.,\n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1,\n",
    "                shear_range = 0.07,\n",
    "                zoom_range = 0.2,\n",
    "                validation_split = validation_split, # <- specify validation_split ratio\n",
    "                # fill_mode='constant',\n",
    "                # cval=0.,\n",
    "                # rescale=1.0/255.0,\n",
    "                rescale=1.0/255.0,\n",
    "                )\n",
    "\n",
    "# create parameter\n",
    "pars = dict(\n",
    "                # basic information\n",
    "                timestamp = timestamp,\n",
    "                date = date,\n",
    "                seed = seed,\n",
    "                batch_size = batch_size,\n",
    "                \n",
    "                # Data generator\n",
    "                crop_input_set = crop_input_set,\n",
    "                validation_steps = validation_steps,\n",
    "                validation_split = validation_split,\n",
    "                training_sample_size = training_sample_size,\n",
    "                \n",
    "                # training class\n",
    "                classes = classes,\n",
    "                inputclass = inputclass,\n",
    "    \n",
    "                # add datagen args\n",
    "                data_gen_img_args = data_gen_img_args,\n",
    "                data_gen_label_args = data_gen_label_args,\n",
    "                \n",
    "                # Build model\n",
    "                IMG_HEIGHT = IMG_HEIGHT,\n",
    "                IMG_WIDTH = IMG_WIDTH,\n",
    "                epoch = epoch, \n",
    "                loss = loss,\n",
    "                metrics_name = metrics_name,\n",
    "                learning_rate = learning_rate,\n",
    "                )\n",
    "\n",
    "# save parameter\n",
    "path_pars = os.path.join(path_logs, 'pars')\n",
    "dir_checker(inputclass[0], path_pars)\n",
    "dir_checker(date, os.path.join(path_pars, inputclass[0]))\n",
    "\n",
    "pprint(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:PerlmutterData\\logs\\pars\\nucleus\\2020_02_05\\pars_2020_02_05_16_27.json\n"
     ]
    }
   ],
   "source": [
    "par_file_dir = os.path.join(path_pars, inputclass[0], date, 'pars_' + timestamp + '.json')\n",
    "print(par_file_dir)\n",
    "\n",
    "with open(par_file_dir, 'w') as outfile:\n",
    "    json.dump(pars, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator\n",
    "image_datagen = ImageDataGenerator(**data_gen_img_args)\n",
    "label_datagen = ImageDataGenerator(**data_gen_label_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232 images belonging to 1 classes.\n",
      "Found 232 images belonging to 1 classes.\n",
      "Found 99 images belonging to 1 classes.\n",
      "Found 99 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# load images into generator\n",
    "train_image_generator = image_datagen.flow_from_directory(\n",
    "    img_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='training', # <- define subset as 'training'\n",
    "    seed=seed)\n",
    "\n",
    "train_label_generator = label_datagen.flow_from_directory(\n",
    "    label_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "valid_image_generator = image_datagen.flow_from_directory(\n",
    "    img_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation', # <- define subset as 'validation'\n",
    "    seed=seed)\n",
    "\n",
    "valid_label_generator = label_datagen.flow_from_directory(\n",
    "    label_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge image and label generator\n",
    "def combine_generator(gen1, gen2):\n",
    "    while True:\n",
    "        yield(gen1.next(), gen2.next()) \n",
    "train_generator = combine_generator(train_image_generator, train_label_generator)\n",
    "valid_generator = combine_generator(valid_image_generator, valid_label_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_generator = zip(train_image_generator, train_label_generator)\\nvalid_generator = zip(valid_image_generator, valid_label_generator)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_generator = zip(train_image_generator, train_label_generator)\n",
    "valid_generator = zip(valid_image_generator, valid_label_generator)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 28.0\n"
     ]
    }
   ],
   "source": [
    "# calculate steps_per_epoch\n",
    "steps_per_epoch = training_sample_size * (1 - validation_split) // batch_size\n",
    "print(\"Steps per epoch: {}\".format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "# Create a .v2 file for saving hyperparameter and evaluation\n",
    "# so we can see the results on tensorboard\n",
    "hparamtuning_dir = os.path.join(path_logs, 'fit', inputclass[0], date, timestamp)\n",
    "\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.7, 0.5]))\n",
    "HP_LAYERS = hp.HParam('layers', hp.Discrete([5, 4]))\n",
    "\n",
    "# hparams_list = [HP_DROPOUT, HP_LAYERS]\n",
    "\n",
    "with tf.summary.create_file_writer(hparamtuning_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_DROPOUT, HP_LAYERS],\n",
    "        metrics=[hp.Metric('accuracy', display_name='Accuracy'), \n",
    "                 hp.Metric('iou_coef', display_name='IoU_Coef'), # create container for customized metrics\n",
    "                 hp.Metric('dice_coef', display_name='Dice_Coef')], # the same\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_name, hparamtuning_dir, hparams):\n",
    "    \n",
    "    # checkpoint\n",
    "    modelfilename = 'model_' + timestamp + '.h5'\n",
    "    dir_checker(run_name, hparamtuning_dir)\n",
    "    dir_checker('model', os.path.join(hparamtuning_dir, run_name))\n",
    "    \n",
    "    modelfile_path = os.path.join(hparamtuning_dir, run_name, 'model', modelfilename)\n",
    "    checkpointer = ModelCheckpoint(filepath = modelfile_path, \n",
    "                                   monitor = 'val_accuracy', \n",
    "                                   mode = 'max', \n",
    "                                   verbose = 1, \n",
    "                                   save_best_only = True, \n",
    "                                  )\n",
    "\n",
    "    # early stopping \n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=8,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4)\n",
    "\n",
    "    # learning rate adjustment\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                        factor=0.1,\n",
    "                        patience=4,\n",
    "                        verbose=1,\n",
    "                        min_delta=1e-4)\n",
    "\n",
    "    # tensorboard ----------------------------------------------\n",
    "    \n",
    "    # file_writer = create_file_writer(os.path.join(path_logs, 'fit', inputclass[0], date, timestamp, \"metrics\"))\n",
    "    # file_writer.set_as_default()\n",
    "\n",
    "    metrics = ['accuracy', iou_coef, dice_coef]\n",
    "    \n",
    "    tensorboard_callback = TensorBoard(log_dir = os.path.join(hparamtuning_dir, run_name), \n",
    "                                       profile_batch = 0, \n",
    "                                       update_freq= 30,\n",
    "                                       histogram_freq = 1\n",
    "                                       )\n",
    "\n",
    "    # compile callbacks\n",
    "    # callbacks = [checkpointer, tensorboard_callback, early_stopping, reduceLR]\n",
    "    callbacks = [checkpointer, reduceLR, tensorboard_callback]\n",
    "    \n",
    "    hparamtuning_runname_dir = os.path.join(hparamtuning_dir, run_name)\n",
    "    \n",
    "    \n",
    "    with tf.summary.create_file_writer(hparamtuning_runname_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "\n",
    "        # prepare the model -----------------------------------\n",
    "        \n",
    "        # load hyper-parameter\n",
    "        dropout = float(hparams[HP_DROPOUT])\n",
    "        print('dropout: {}'.format(dropout))\n",
    "        \n",
    "        num_layers = int(hparams[HP_LAYERS])\n",
    "        print('num layers: {}'.format(num_layers))\n",
    "        \n",
    "        unetmodel = vanilla_unet_nodrop(\n",
    "                            shape = (IMG_HEIGHT, IMG_WIDTH), \n",
    "                            dropout = dropout, \n",
    "                            num_layers = num_layers, \n",
    "                            lr = learning_rate, \n",
    "                            loss = loss,\n",
    "                            metrics = metrics,\n",
    "                            summary = False,\n",
    "                           )\n",
    "        \n",
    "        # load model ------------------------------------------\n",
    "        \n",
    "        '''\n",
    "        # load weight\n",
    "        path_model = os.path.join('D:', 'PerlmutterData', 'logs', 'model', \n",
    "                                'nucleus', \n",
    "                                '2019_12_13',\n",
    "                                '2019_12_13_17_28',\n",
    "                                'run-1', \n",
    "                                'model_2019_12_13_17_28.h5',)\n",
    "        \n",
    "        unetmodel.load_weights(path_model)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # train the model -------------------------------------\n",
    "        unetmodel.fit_generator(\n",
    "                            generator = train_generator, \n",
    "                            validation_data = valid_generator,\n",
    "                            validation_steps = validation_steps,\n",
    "                            steps_per_epoch = steps_per_epoch,\n",
    "                            epochs = epoch,  \n",
    "                            callbacks = callbacks,\n",
    "                            verbose = 1, \n",
    "                            )\n",
    "    \n",
    "        _, accuracy, iou, dice,  = unetmodel.evaluate_generator(valid_generator, steps = 50, verbose=1)\n",
    "        tf.summary.scalar('accuracy', accuracy, step = 1)\n",
    "        tf.summary.scalar('iou_coef', iou, step = 1)\n",
    "        tf.summary.scalar('dice_coef', dice, step = 1)\n",
    "        \n",
    "        # -----------------------------------------------------\n",
    "        \n",
    "        # clean memory\n",
    "        K.clear_session()\n",
    "        del unetmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{HParam(name='dropout', domain=Discrete([0.5, 0.7]), display_name=None, description=None): 0.5, HParam(name='layers', domain=Discrete([4, 5]), display_name=None, description=None): 4}\n",
      "{'dropout': 0.5, 'layers': 4}\n",
      "run-0 does not exist in D:PerlmutterData\\logs\\fit\\nucleus\\2020_02_05\\2020_02_05_16_27\n",
      "model does not exist in D:PerlmutterData\\logs\\fit\\nucleus\\2020_02_05\\2020_02_05_16_27\\run-0\n",
      "dropout: 0.5\n",
      "num layers: 4\n",
      "Epoch 1/20\n",
      "27/28 [===========================>..] - ETA: 1s - loss: 0.6536 - accuracy: 0.6787 - iou_coef: 0.1004 - dice_coef: 0.1549\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.56742, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2020_02_05\\2020_02_05_16_27\\run-0\\model\\model_2020_02_05_16_27.h5\n",
      "28/28 [==============================] - 45s 2s/step - loss: 0.6523 - accuracy: 0.6790 - iou_coef: 0.1008 - dice_coef: 0.1555 - val_loss: 0.7062 - val_accuracy: 0.5674 - val_iou_coef: 0.1500 - val_dice_coef: 0.2366\n",
      "Epoch 2/20\n",
      "27/28 [===========================>..] - ETA: 0s - loss: 0.5926 - accuracy: 0.7204 - iou_coef: 0.0930 - dice_coef: 0.1477\n",
      "Epoch 00002: val_accuracy improved from 0.56742 to 0.58194, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2020_02_05\\2020_02_05_16_27\\run-0\\model\\model_2020_02_05_16_27.h5\n",
      "28/28 [==============================] - 33s 1s/step - loss: 0.5931 - accuracy: 0.7190 - iou_coef: 0.0934 - dice_coef: 0.1482 - val_loss: 0.6843 - val_accuracy: 0.5819 - val_iou_coef: 0.1391 - val_dice_coef: 0.2210\n",
      "Epoch 3/20\n",
      "12/28 [===========>..................] - ETA: 13s - loss: 0.5939 - accuracy: 0.7077 - iou_coef: 0.1102 - dice_coef: 0.1727"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method EagerResourceDeleter.__del__ of <tensorflow.python.ops.resource_variable_ops.EagerResourceDeleter object at 0x00000226FB3ED518>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\", line 273, in __del__\n",
      "    self._handle, ignore_lookup_error=True)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\", line 311, in destroy_resource_op\n",
      "    \"ignore_lookup_error\", ignore_lookup_error)\n",
      "KeyboardInterrupt: \n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-344834354618>\", line 19, in <module>\n",
      "    run(run_name, hparamtuning_dir, hparams)\n",
      "  File \"<ipython-input-16-358a8e36ead6>\", line 94, in run\n",
      "    verbose = 1,\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1297, in fit_generator\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 265, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 973, in train_on_batch\n",
      "    class_weight=class_weight, reset_metrics=reset_metrics)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 264, in train_on_batch\n",
      "    output_loss_metrics=model._output_loss_metrics)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 311, in train_on_batch\n",
      "    output_loss_metrics=output_loss_metrics))\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 268, in _process_single_batch\n",
      "    grads = tape.gradient(scaled_total_loss, trainable_weights)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\", line 1014, in gradient\n",
      "    unconnected_gradients=unconnected_gradients)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\", line 76, in imperative_grad\n",
      "    compat.as_str(unconnected_gradients.value))\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\", line 138, in _gradient_function\n",
      "    return grad_fn(mock_op, *out_grads)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_grad.py\", line 578, in _Conv2DGrad\n",
      "    shape_0, shape_1 = array_ops.shape_n([op.inputs[0], op.inputs[1]])\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 505, in shape_n\n",
      "    return gen_array_ops.shape_n(input, out_type=out_type, name=name)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 9034, in shape_n\n",
      "    name, _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\wucci_admin\\Anaconda3\\envs\\tfdl02\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "session_num = 0\n",
    "\n",
    "for dropout in HP_DROPOUT.domain.values:\n",
    "    for layer in HP_LAYERS.domain.values:\n",
    "        \n",
    "        run_name = \"run-{}\".format(session_num)\n",
    "        print('--- Starting trial: {}'.format(run_name))\n",
    "        \n",
    "        # create hyper-parameter\n",
    "        hparams = {\n",
    "            HP_DROPOUT: dropout,\n",
    "            HP_LAYERS: layer,\n",
    "        }\n",
    "        print(hparams)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "        # build model and traning\n",
    "        run(run_name, hparamtuning_dir, hparams)\n",
    "        \n",
    "        session_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "tfdl02",
   "language": "python",
   "name": "tfdl02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
