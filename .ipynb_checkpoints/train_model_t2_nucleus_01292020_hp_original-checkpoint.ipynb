{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model\n",
    "1. Keras on Tensorflow\n",
    "2. hyper-parameter search\n",
    "3. vanila-Unet\n",
    "4. date: 01292020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import tensorflow as tf\n",
    "from skimage.io import imread, imsave, imshow\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import itertools\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from core.imageprep import dir_checker, random_crop, crop_generator, random_crop_batch\n",
    "from core.models import UNet, UNet_hp, vanilla_unet, vanilla_unet_hp\n",
    "from core.metrics import iou_coef, dice_coef\n",
    "\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython import get_ipython\n",
    "# %load_ext autoreload\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "# %autoreload 2\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "# %load_ext tensorboard\n",
    "get_ipython().run_line_magic('load_ext', 'tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "from packaging import version\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wucci_admin\\\\Anaconda3\\\\envs\\\\tfdl02\\\\python.exe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Images...\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_23_09_51_20x\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_23_09_51_20x\\images\n",
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_23_09_51_20x\\labels\n",
      "logs exists in D:PerlmutterData\n",
      "fit exists in D:PerlmutterData\\logs\n",
      "model exists in D:PerlmutterData\\logs\n",
      "pars exists in D:PerlmutterData\\logs\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "print(\"Load Images...\")\n",
    "# on mac\n",
    "# path = \"/Volumes/LaCie_DataStorage/PerlmutterData/\"\n",
    "\n",
    "# on Window PC \n",
    "path = os.path.join('D:', 'PerlmutterData')\n",
    "\n",
    "# input set\n",
    "crop_input_set = '2020_01_23_09_51_20x'\n",
    "# crop_input_set = '2019_12_10_15_33_5x' # small training set\n",
    "\n",
    "imginput = os.path.join('dl_seg_project_raw', 'data_crop', crop_input_set,)\n",
    "imgpath = os.path.join(path, imginput)\n",
    "\n",
    "print(imgpath)\n",
    "\n",
    "img_dir = os.path.join(imgpath, 'images')\n",
    "label_dir = os.path.join(imgpath, 'labels')\n",
    "print(img_dir)\n",
    "print(label_dir)\n",
    "\n",
    "# check if the output folder exist. If not, create a folder\n",
    "dir_checker('logs', path)\n",
    "path_logs = os.path.join(path, 'logs')\n",
    "dir_checker('fit', path_logs)\n",
    "dir_checker('model', path_logs)\n",
    "dir_checker('pars', path_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Print the first file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:PerlmutterData\\dl_seg_project_raw\\data_crop\\2020_01_23_09_51_20x\\images\\nucleus\\0001.tif\n"
     ]
    }
   ],
   "source": [
    "imgpath_all = list(paths.list_images(imgpath))\n",
    "print(imgpath_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create Image Datagenerator\n",
    " 1. create only one datagen\n",
    " 2. specify valiation split in datagen argument\n",
    " 3. add split data when calling `datagen.flow_from_directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M\")\n",
    "date =  datetime.now().strftime(\"%Y_%m_%d\")\n",
    "seed = 102\n",
    "batch_size = 16\n",
    "epoch = 20\n",
    "validation_steps = 20\n",
    "validation_split = 0.3\n",
    "training_sample_size = len(imgpath_all)\n",
    "IMG_HEIGHT = None\n",
    "IMG_WIDTH = None\n",
    "classes = ['cell_membrane', 'nucleus', 'autophagosome']\n",
    "inputclass = [classes[1]]\n",
    "learning_rate = 1e-4\n",
    "loss = \"binary_crossentropy\"\n",
    "metrics = ['accuracy', iou_coef, dice_coef]\n",
    "\n",
    "metrics_name = []\n",
    "for f in metrics:\n",
    "    if callable(f):\n",
    "        metrics_name.append(f.__name__)\n",
    "    else:\n",
    "        metrics_name.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nucleus exists in D:PerlmutterData\\logs\\pars\n",
      "2020_01_29 exists in D:PerlmutterData\\logs\\pars\\nucleus\n",
      "{'IMG_HEIGHT': None,\n",
      " 'IMG_WIDTH': None,\n",
      " 'batch_size': 16,\n",
      " 'classes': ['cell_membrane', 'nucleus', 'autophagosome'],\n",
      " 'crop_input_set': '2020_01_23_09_51_20x',\n",
      " 'data_gen_img_args': {'height_shift_range': 0.1,\n",
      "                       'horizontal_flip': True,\n",
      "                       'rescale': 0.00392156862745098,\n",
      "                       'rotation_range': 90.0,\n",
      "                       'shear_range': 0.07,\n",
      "                       'validation_split': 0.3,\n",
      "                       'vertical_flip': True,\n",
      "                       'width_shift_range': 0.1,\n",
      "                       'zoom_range': 0.2},\n",
      " 'data_gen_label_args': {'height_shift_range': 0.1,\n",
      "                         'horizontal_flip': True,\n",
      "                         'rescale': 0.00392156862745098,\n",
      "                         'rotation_range': 90.0,\n",
      "                         'shear_range': 0.07,\n",
      "                         'validation_split': 0.3,\n",
      "                         'vertical_flip': True,\n",
      "                         'width_shift_range': 0.1,\n",
      "                         'zoom_range': 0.2},\n",
      " 'date': '2020_01_29',\n",
      " 'epoch': 20,\n",
      " 'inputclass': ['nucleus'],\n",
      " 'learning_rate': 0.0001,\n",
      " 'loss': 'binary_crossentropy',\n",
      " 'metrics_name': ['accuracy', 'iou_coef', 'dice_coef'],\n",
      " 'seed': 102,\n",
      " 'timestamp': '2020_01_29_14_20',\n",
      " 'training_sample_size': 13240,\n",
      " 'validation_split': 0.3,\n",
      " 'validation_steps': 20}\n"
     ]
    }
   ],
   "source": [
    "# create arguments for data generator\n",
    "data_gen_img_args = dict(\n",
    "                # featurewise_center = True,\n",
    "                # featurewise_std_normalization = True,\n",
    "                horizontal_flip = True,\n",
    "                vertical_flip = True,\n",
    "                rotation_range = 90.,\n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1,\n",
    "                shear_range = 0.07,\n",
    "                zoom_range = 0.2,\n",
    "                validation_split = validation_split, # <- specify validation_split ratio\n",
    "                # fill_mode='constant',\n",
    "                # cval=0.,\n",
    "                rescale=1.0/255.0,\n",
    "                )\n",
    "\n",
    "data_gen_label_args = dict(\n",
    "                # featurewise_center=True,\n",
    "                # featurewise_std_normalization=True,\n",
    "                horizontal_flip = True,\n",
    "                vertical_flip = True,\n",
    "                rotation_range = 90.,\n",
    "                width_shift_range = 0.1,\n",
    "                height_shift_range = 0.1,\n",
    "                shear_range = 0.07,\n",
    "                zoom_range = 0.2,\n",
    "                validation_split = validation_split, # <- specify validation_split ratio\n",
    "                # fill_mode='constant',\n",
    "                # cval=0.,\n",
    "                # rescale=1.0/255.0,\n",
    "                rescale=1.0/255.0,\n",
    "                )\n",
    "\n",
    "# create parameter\n",
    "pars = dict(\n",
    "                # basic information\n",
    "                timestamp = timestamp,\n",
    "                date = date,\n",
    "                seed = seed,\n",
    "                batch_size = batch_size,\n",
    "                \n",
    "                # Data generator\n",
    "                crop_input_set = crop_input_set,\n",
    "                validation_steps = validation_steps,\n",
    "                validation_split = validation_split,\n",
    "                training_sample_size = training_sample_size,\n",
    "                \n",
    "                # training class\n",
    "                classes = classes,\n",
    "                inputclass = inputclass,\n",
    "    \n",
    "                # add datagen args\n",
    "                data_gen_img_args = data_gen_img_args,\n",
    "                data_gen_label_args = data_gen_label_args,\n",
    "                \n",
    "                # Build model\n",
    "                IMG_HEIGHT = IMG_HEIGHT,\n",
    "                IMG_WIDTH = IMG_WIDTH,\n",
    "                epoch = epoch, \n",
    "                loss = loss,\n",
    "                metrics_name = metrics_name,\n",
    "                learning_rate = learning_rate,\n",
    "                )\n",
    "\n",
    "# save parameter\n",
    "path_pars = os.path.join(path_logs, 'pars')\n",
    "dir_checker(inputclass[0], path_pars)\n",
    "dir_checker(date, os.path.join(path_pars, inputclass[0]))\n",
    "\n",
    "pprint(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:PerlmutterData\\logs\\pars\\nucleus\\2020_01_29\\pars_2020_01_29_14_20.json\n"
     ]
    }
   ],
   "source": [
    "par_file_dir = os.path.join(path_pars, inputclass[0], date, 'pars_' + timestamp + '.json')\n",
    "print(par_file_dir)\n",
    "\n",
    "with open(par_file_dir, 'w') as outfile:\n",
    "    json.dump(pars, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generator\n",
    "image_datagen = ImageDataGenerator(**data_gen_img_args)\n",
    "label_datagen = ImageDataGenerator(**data_gen_label_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4634 images belonging to 1 classes.\n",
      "Found 4634 images belonging to 1 classes.\n",
      "Found 1986 images belonging to 1 classes.\n",
      "Found 1986 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# load images into generator\n",
    "train_image_generator = image_datagen.flow_from_directory(\n",
    "    img_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='training', # <- define subset as 'training'\n",
    "    seed=seed)\n",
    "\n",
    "train_label_generator = label_datagen.flow_from_directory(\n",
    "    label_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "valid_image_generator = image_datagen.flow_from_directory(\n",
    "    img_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation', # <- define subset as 'validation'\n",
    "    seed=seed)\n",
    "\n",
    "valid_label_generator = label_datagen.flow_from_directory(\n",
    "    label_dir,\n",
    "    class_mode=None,\n",
    "    classes=inputclass,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge image and label generator\n",
    "def combine_generator(gen1, gen2):\n",
    "    while True:\n",
    "        yield(gen1.next(), gen2.next()) \n",
    "train_generator = combine_generator(train_image_generator, train_label_generator)\n",
    "valid_generator = combine_generator(valid_image_generator, valid_label_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_generator = zip(train_image_generator, train_label_generator)\\nvalid_generator = zip(valid_image_generator, valid_label_generator)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_generator = zip(train_image_generator, train_label_generator)\n",
    "valid_generator = zip(valid_image_generator, valid_label_generator)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Define Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 579.0\n"
     ]
    }
   ],
   "source": [
    "# calculate steps_per_epoch\n",
    "steps_per_epoch = training_sample_size * (1 - validation_split) // batch_size\n",
    "print(\"Steps per epoch: {}\".format(steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "# Create a .v2 file for saving hyperparameter and evaluation\n",
    "# so we can see the results on tensorboard\n",
    "hparamtuning_dir = os.path.join(path_logs, 'fit', inputclass[0], date, timestamp)\n",
    "\n",
    "HP_DROPOUT_1 = hp.HParam('dropout_1', hp.Discrete([0.3, 0.5]))\n",
    "HP_DROPOUT_2 = hp.HParam('dropout_2', hp.Discrete([0.5, 0.7]))\n",
    "HP_LAYERS = hp.HParam('layers', hp.Discrete([5, 4, 3]))\n",
    "\n",
    "hparams_list = [HP_DROPOUT_1, HP_DROPOUT_2, HP_LAYERS]\n",
    "\n",
    "with tf.summary.create_file_writer(hparamtuning_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_DROPOUT_1, HP_DROPOUT_2, HP_LAYERS],\n",
    "    metrics=[hp.Metric('accuracy', display_name='Accuracy'), \n",
    "             hp.Metric('iou_coef', display_name='IoU_Coef'), # create container for customized metrics\n",
    "             hp.Metric('dice_coef', display_name='Dice_Coef')], # the same\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_name, hparamtuning_dir, hparams):\n",
    "    \n",
    "    # checkpoint\n",
    "    modelfilename = 'model_' + timestamp + '.h5'\n",
    "    dir_checker(run_name, hparamtuning_dir)\n",
    "    dir_checker('model', os.path.join(hparamtuning_dir, run_name))\n",
    "    \n",
    "    modelfile_path = os.path.join(hparamtuning_dir, run_name, 'model', modelfilename)\n",
    "    checkpointer = ModelCheckpoint(filepath = modelfile_path, \n",
    "                                   monitor = 'val_loss', \n",
    "                                   mode = 'min', \n",
    "                                   verbose = 1, \n",
    "                                   save_best_only = True)\n",
    "\n",
    "    # early stopping \n",
    "    early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=8,\n",
    "                               verbose=1,\n",
    "                               min_delta=1e-4)\n",
    "\n",
    "    # learning rate adjustment\n",
    "    reduceLR = ReduceLROnPlateau(monitor='val_loss',\n",
    "                        factor=0.1,\n",
    "                        patience=4,\n",
    "                        verbose=1,\n",
    "                        min_delta=1e-4)\n",
    "\n",
    "    # tensorboard\n",
    "    # file_writer = create_file_writer(os.path.join(path_logs, 'fit', inputclass[0], date, timestamp, \"metrics\"))\n",
    "    # file_writer.set_as_default()\n",
    "\n",
    "    metrics = ['accuracy', iou_coef, dice_coef]\n",
    "    \n",
    "    tensorboard_callback = TensorBoard(log_dir = os.path.join(hparamtuning_dir, run_name), \n",
    "                                       profile_batch = 0, \n",
    "                                       update_freq= 30,\n",
    "                                       histogram_freq = 1\n",
    "                                       )\n",
    "\n",
    "    # compile callbacks\n",
    "    # callbacks = [checkpointer, tensorboard_callback, early_stopping, reduceLR]\n",
    "    callbacks = [checkpointer, reduceLR, tensorboard_callback]\n",
    "    \n",
    "    hparamtuning_runname_dir = os.path.join(hparamtuning_dir, run_name)\n",
    "    \n",
    "    with tf.summary.create_file_writer(hparamtuning_runname_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "\n",
    "        # prepare the model\n",
    "        unetmodel = vanilla_unet_hp(\n",
    "                            shape = (IMG_HEIGHT, IMG_WIDTH), \n",
    "                            lr = learning_rate, \n",
    "                            loss = loss,\n",
    "                            metrics = metrics,\n",
    "                            hparams = hparams,\n",
    "                            hparams_list = hparams_list, \n",
    "                           )\n",
    "        '''\n",
    "        # load weight\n",
    "        path_model = os.path.join('D:', 'PerlmutterData', 'logs', 'model', \n",
    "                                'nucleus', \n",
    "                                '2019_12_13',\n",
    "                                '2019_12_13_17_28',\n",
    "                                'run-1', \n",
    "                                'model_2019_12_13_17_28.h5',)\n",
    "        \n",
    "        unetmodel.load_weights(path_model)\n",
    "        '''\n",
    "        \n",
    "        # train the model\n",
    "        unetmodel.fit_generator(\n",
    "                            generator = train_generator, \n",
    "                            validation_data = valid_generator,\n",
    "                            validation_steps = validation_steps,\n",
    "                            steps_per_epoch = steps_per_epoch,\n",
    "                            epochs = epoch,  \n",
    "                            callbacks = callbacks,\n",
    "                            verbose = 1, \n",
    "                            )\n",
    "    \n",
    "        _, accuracy, iou, dice,  = unetmodel.evaluate_generator(valid_generator, steps = 50, verbose=1)\n",
    "        tf.summary.scalar('accuracy', accuracy, step = 1)\n",
    "        tf.summary.scalar('iou_coef', iou, step = 1)\n",
    "        tf.summary.scalar('dice_coef', dice, step = 1)\n",
    "        \n",
    "        # clean memory\n",
    "        K.clear_session()\n",
    "        del unetmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{HParam(name='dropout_1', domain=Discrete([0.3, 0.5]), display_name=None, description=None): 0.3, HParam(name='dropout_2', domain=Discrete([0.5, 0.7]), display_name=None, description=None): 0.5, HParam(name='layers', domain=Discrete([3, 4, 5]), display_name=None, description=None): 3}\n",
      "--- Starting trial: run-0\n",
      "{'dropout_1': 0.3, 'dropout_2': 0.5, 'layers': 3}\n",
      "run-0 does not exist in D:PerlmutterData\\logs\\fit\\nucleus\\2020_01_29\\2020_01_29_14_20\n",
      "model does not exist in D:PerlmutterData\\logs\\fit\\nucleus\\2020_01_29\\2020_01_29_14_20\\run-0\n",
      "input: (None, None, 1)\n",
      "dropout_1: 0.3\n",
      "dropout_2: 0.5\n",
      "num layers: 3\n",
      "encoder\n",
      "block 1\n",
      "input: (None, None, None, 64)\n",
      "encoder\n",
      "block 2\n",
      "input: (None, None, None, 128)\n",
      "encoder\n",
      "block 3\n",
      "input: (None, None, None, 256)\n",
      "512\n",
      "input: (None, None, None, 512)\n",
      "decoder\n",
      "input_conv: (None, None, None, 256)\n",
      "input_x: (None, None, None, 256)\n",
      "decoder\n",
      "input_conv: (None, None, None, 128)\n",
      "input_x: (None, None, None, 128)\n",
      "decoder\n",
      "input_conv: (None, None, None, 64)\n",
      "input_x: (None, None, None, 64)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 6 640         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, None, 6 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 6 36928       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, None, None, 6 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 1 73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, None, 1 0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 147584      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 2 295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, None, 2 0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 2 590080      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 2 0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, None, 2 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 5 1180160     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, None, 5 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 5 2359808     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, None, None, 5 0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 2 524544      up_sampling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 5 0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 2 1179904     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, None, 2 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 2 590080      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, None, None, 2 0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 1 131200      up_sampling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 2 0           conv2d_11[0][0]                  \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 1 295040      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, None, 1 0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 147584      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, None, None, 1 0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 6 32832       up_sampling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 1 0           conv2d_14[0][0]                  \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 6 73792       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, None, 6 0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 6 36928       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 1 65          conv2d_16[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,696,193\n",
      "Trainable params: 7,696,193\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "578/579 [============================>.] - ETA: 1s - loss: 0.6052 - accuracy: 0.7207 - iou_coef: 0.0939 - dice_coef: 0.1493\n",
      "Epoch 00001: val_loss improved from inf to 0.71046, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2020_01_29\\2020_01_29_14_20\\run-0\\model\\model_2020_01_29_14_20.h5\n",
      "579/579 [==============================] - 614s 1s/step - loss: 0.6051 - accuracy: 0.7207 - iou_coef: 0.0939 - dice_coef: 0.1493 - val_loss: 0.7105 - val_accuracy: 0.5432 - val_iou_coef: 0.1752 - val_dice_coef: 0.2661\n",
      "Epoch 2/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.5656 - accuracy: 0.7237 - iou_coef: 0.1000 - dice_coef: 0.1566\n",
      "Epoch 00002: val_loss improved from 0.71046 to 0.63782, saving model to D:PerlmutterData\\logs\\fit\\nucleus\\2020_01_29\\2020_01_29_14_20\\run-0\\model\\model_2020_01_29_14_20.h5\n",
      "579/579 [==============================] - 605s 1s/step - loss: 0.5659 - accuracy: 0.7233 - iou_coef: 0.1002 - dice_coef: 0.1568 - val_loss: 0.6378 - val_accuracy: 0.5906 - val_iou_coef: 0.1816 - val_dice_coef: 0.2724\n",
      "Epoch 3/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.5305 - accuracy: 0.7248 - iou_coef: 0.1100 - dice_coef: 0.1682\n",
      "Epoch 00003: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 603s 1s/step - loss: 0.5305 - accuracy: 0.7248 - iou_coef: 0.1101 - dice_coef: 0.1683 - val_loss: 0.6929 - val_accuracy: 0.5602 - val_iou_coef: 0.1574 - val_dice_coef: 0.2468\n",
      "Epoch 4/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.4518 - accuracy: 0.8017 - iou_coef: 0.1534 - dice_coef: 0.2079\n",
      "Epoch 00004: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 602s 1s/step - loss: 0.4517 - accuracy: 0.8018 - iou_coef: 0.1536 - dice_coef: 0.2081 - val_loss: 0.6626 - val_accuracy: 0.5964 - val_iou_coef: 0.1435 - val_dice_coef: 0.2257\n",
      "Epoch 5/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.3410 - accuracy: 0.8705 - iou_coef: 0.2107 - dice_coef: 0.2589\n",
      "Epoch 00005: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 602s 1s/step - loss: 0.3408 - accuracy: 0.8706 - iou_coef: 0.2105 - dice_coef: 0.2587 - val_loss: 0.6803 - val_accuracy: 0.5880 - val_iou_coef: 0.1760 - val_dice_coef: 0.2669\n",
      "Epoch 6/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2801 - accuracy: 0.8990 - iou_coef: 0.2372 - dice_coef: 0.2811\n",
      "Epoch 00006: val_loss did not improve from 0.63782\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "579/579 [==============================] - 601s 1s/step - loss: 0.2801 - accuracy: 0.8990 - iou_coef: 0.2371 - dice_coef: 0.2810 - val_loss: 0.6535 - val_accuracy: 0.6511 - val_iou_coef: 0.1823 - val_dice_coef: 0.2674\n",
      "Epoch 7/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9185 - iou_coef: 0.2594 - dice_coef: 0.3004\n",
      "Epoch 00007: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 598s 1s/step - loss: 0.2341 - accuracy: 0.9185 - iou_coef: 0.2594 - dice_coef: 0.3004 - val_loss: 0.8186 - val_accuracy: 0.5982 - val_iou_coef: 0.1427 - val_dice_coef: 0.2206\n",
      "Epoch 8/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9212 - iou_coef: 0.2623 - dice_coef: 0.3020\n",
      "Epoch 00008: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 598s 1s/step - loss: 0.2278 - accuracy: 0.9211 - iou_coef: 0.2623 - dice_coef: 0.3020 - val_loss: 0.8886 - val_accuracy: 0.5762 - val_iou_coef: 0.1340 - val_dice_coef: 0.2110\n",
      "Epoch 9/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2257 - accuracy: 0.9219 - iou_coef: 0.2627 - dice_coef: 0.3018\n",
      "Epoch 00009: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 598s 1s/step - loss: 0.2256 - accuracy: 0.9220 - iou_coef: 0.2629 - dice_coef: 0.3020 - val_loss: 0.9000 - val_accuracy: 0.5807 - val_iou_coef: 0.1174 - val_dice_coef: 0.1886\n",
      "Epoch 10/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2219 - accuracy: 0.9233 - iou_coef: 0.2651 - dice_coef: 0.3034\n",
      "Epoch 00010: val_loss did not improve from 0.63782\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "579/579 [==============================] - 599s 1s/step - loss: 0.2219 - accuracy: 0.9233 - iou_coef: 0.2654 - dice_coef: 0.3037 - val_loss: 0.8386 - val_accuracy: 0.6058 - val_iou_coef: 0.1116 - val_dice_coef: 0.1792\n",
      "Epoch 11/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9248 - iou_coef: 0.2637 - dice_coef: 0.3028\n",
      "Epoch 00011: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 597s 1s/step - loss: 0.2188 - accuracy: 0.9247 - iou_coef: 0.2637 - dice_coef: 0.3028 - val_loss: 0.8414 - val_accuracy: 0.6059 - val_iou_coef: 0.1265 - val_dice_coef: 0.2029\n",
      "Epoch 12/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2192 - accuracy: 0.9245 - iou_coef: 0.2671 - dice_coef: 0.3057\n",
      "Epoch 00012: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 598s 1s/step - loss: 0.2194 - accuracy: 0.9244 - iou_coef: 0.2669 - dice_coef: 0.3055 - val_loss: 0.8974 - val_accuracy: 0.5817 - val_iou_coef: 0.1168 - val_dice_coef: 0.1874\n",
      "Epoch 13/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2164 - accuracy: 0.9258 - iou_coef: 0.2691 - dice_coef: 0.3073\n",
      "Epoch 00013: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 599s 1s/step - loss: 0.2165 - accuracy: 0.9257 - iou_coef: 0.2691 - dice_coef: 0.3073 - val_loss: 0.8418 - val_accuracy: 0.6074 - val_iou_coef: 0.1030 - val_dice_coef: 0.1693\n",
      "Epoch 14/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9243 - iou_coef: 0.2679 - dice_coef: 0.3062\n",
      "Epoch 00014: val_loss did not improve from 0.63782\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "579/579 [==============================] - 599s 1s/step - loss: 0.2200 - accuracy: 0.9240 - iou_coef: 0.2678 - dice_coef: 0.3061 - val_loss: 0.9320 - val_accuracy: 0.5629 - val_iou_coef: 0.1120 - val_dice_coef: 0.1822\n",
      "Epoch 15/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2159 - accuracy: 0.9255 - iou_coef: 0.2659 - dice_coef: 0.3041\n",
      "Epoch 00015: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 599s 1s/step - loss: 0.2157 - accuracy: 0.9256 - iou_coef: 0.2660 - dice_coef: 0.3042 - val_loss: 0.7682 - val_accuracy: 0.6466 - val_iou_coef: 0.1048 - val_dice_coef: 0.1681\n",
      "Epoch 16/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9251 - iou_coef: 0.2667 - dice_coef: 0.3049\n",
      "Epoch 00016: val_loss did not improve from 0.63782\n",
      "579/579 [==============================] - 596s 1s/step - loss: 0.2182 - accuracy: 0.9250 - iou_coef: 0.2668 - dice_coef: 0.3049 - val_loss: 0.8595 - val_accuracy: 0.6012 - val_iou_coef: 0.1043 - val_dice_coef: 0.1704\n",
      "Epoch 17/20\n",
      "578/579 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9256 - iou_coef: 0.2694 - dice_coef: 0.3080"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "session_num = 0\n",
    "\n",
    "for dropout_rate_1 in HP_DROPOUT_1.domain.values:\n",
    "    for layer in HP_LAYERS.domain.values:\n",
    "        hparams = {\n",
    "            HP_DROPOUT_1: dropout_rate_1,\n",
    "            HP_DROPOUT_2: dropout_rate_2,\n",
    "            HP_LAYERS: layer,\n",
    "\n",
    "        }\n",
    "        print(hparams)\n",
    "\n",
    "        run_name = \"run-%d\" % session_num\n",
    "        print('--- Starting trial: %s' % run_name)\n",
    "        print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "        run(run_name, hparamtuning_dir, hparams)\n",
    "        session_num += 1\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "tfdl02",
   "language": "python",
   "name": "tfdl02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
